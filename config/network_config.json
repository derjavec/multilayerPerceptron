{
    "layer": [4, 3, 3, 2],
    "activations": ["relu", "relu", "relu", "softmax"],
    "epochs": 5,
    "batch_size": 1,
    "learning_rate": 0.01
}
