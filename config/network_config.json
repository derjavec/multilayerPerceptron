{
    "layer": [32, 16, 16, 2],
    "activations": ["relu", "relu", "relu", "softmax"],
    "epochs": 50,
    "batch_size": 8,
    "learning_rate": 0.01
}
